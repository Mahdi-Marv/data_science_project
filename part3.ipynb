{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part 3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Lenovo\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import tqdm as tqdm\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from transformers import RobertaForSequenceClassification, RobertaTokenizer\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample 100k tweets\n",
    "sample_size = 100000\n",
    "df_sampled = df.sample(n=sample_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preprocess the data (exactly the same function that was used in part 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stemmed = [stemmer.stem(word) for word in tokens]\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    cleaned_text = ' '.join(lemmatized)  \n",
    "    return text\n",
    "\n",
    "\n",
    "df_sampled['text'] = df_sampled['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the Roberta model and run the model on the sampled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the paths to your config.json and model.safetensors files\n",
    "config_path = \"roberta/roberta_model/config.json\"\n",
    "model_path = \"roberta/roberta_model/\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_path, config=config_path)\n",
    "\n",
    "# Define the classify_tweet function\n",
    "def classify_tweet(tweet, tokenizer, model):\n",
    "    inputs = tokenizer(tweet, truncation=True, padding=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_label\n",
    "\n",
    "# Assuming df_sampled is already defined\n",
    "df_sampled['roberta_label'] = df_sampled['text'].apply(lambda tweet: classify_tweet(tweet, tokenizer, model))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the Bert model and run on the sampled data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      text  bert_label\n",
      "4385281  rt unitedhealthgrp unh wichmann unh generated ...           1\n",
      "1425403  nice shake out of weak hands yesterday mms  we...           0\n",
      "2171408  rt momostocktrades vdrm cnbx owcp usrm aapl fb...           0\n",
      "3564549  commercial metals company director just picked...           1\n",
      "7335060  hunting for stocks to short heres one w a low ...           0\n",
      "...                                                    ...         ...\n",
      "7831020  westerndigital ceo in japan to finalize toshib...           1\n",
      "7460314  first financial bankshares inc ffin upgraded t...           0\n",
      "3826266  what moves ftse 100  sentiments analysis or in...           1\n",
      "3831838  must read undervalued microcap about to run ht...           1\n",
      "6790864  tower research capital llc trc sells 81118 sha...           0\n",
      "\n",
      "[100 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Specify the paths to your config.json and model.safetensors files\n",
    "config_path = \"bert/bert_model/config.json\"\n",
    "model_path = \"bert/bert_model/\"\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path, config=config_path)\n",
    "\n",
    "# Define the classify_tweet function\n",
    "def classify_tweet(tweet, tokenizer, model):\n",
    "    inputs = tokenizer(tweet, truncation=True, padding=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_label = torch.argmax(logits, dim=1).item()\n",
    "    return predicted_label\n",
    "\n",
    "# Assuming df_sampled is already defined\n",
    "df_sampled['bert_label'] = df_sampled['text'].apply(lambda tweet: classify_tweet(tweet, tokenizer, model))\n",
    "\n",
    "# Display the results\n",
    "print(df_sampled[['text', 'bert_label']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discuss transfer learning and fine-tuning and how they can be used to improve\n",
    "the overall effect of the project. Use your limited results from the second part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning and Fine-Tuning in Natural Language Processing\n",
    "\n",
    "Transfer learning and fine-tuning are powerful techniques in natural language processing (NLP) that leverage pre-trained models to improve the performance of specific tasks. These techniques can significantly enhance the overall effectiveness of a project by utilizing the knowledge learned from large-scale pre-training datasets.\n",
    "\n",
    "## Transfer Learning\n",
    "\n",
    "Transfer learning involves transferring knowledge from a pre-trained model, trained on a large general-purpose dataset, to a specific task or domain. In NLP, transfer learning typically involves using pre-trained language models such as BERT, RoBERTa, or GPT to initialize the parameters of a model for a downstream task, such as sentiment analysis or text classification.\n",
    "\n",
    "### Benefits of Transfer Learning:\n",
    "\n",
    "1. **Efficient Use of Resources**: Transfer learning allows us to leverage the computational resources and expertise invested in training large-scale pre-trained models, saving time and effort in training from scratch.\n",
    "\n",
    "2. **Improved Performance**: Pre-trained models have learned rich representations of language from vast amounts of text data, which can lead to better performance on downstream tasks, especially when the task has limited labeled data.\n",
    "\n",
    "3. **Domain Adaptation**: Transfer learning enables models to adapt to specific domains or tasks by fine-tuning the pre-trained parameters on domain-specific or task-specific data.\n",
    "\n",
    "## Fine-Tuning\n",
    "\n",
    "Fine-tuning involves further training the pre-trained model on task-specific data to adapt it to the target task. During fine-tuning, the parameters of the pre-trained model are adjusted using task-specific labeled data, typically with a smaller learning rate compared to the initial pre-training phase.\n",
    "\n",
    "### Benefits of Fine-Tuning:\n",
    "\n",
    "1. **Task-Specific Optimization**: Fine-tuning allows the model to learn task-specific patterns and nuances from the labeled data, leading to better performance on the target task.\n",
    "\n",
    "2. **Flexibility**: Fine-tuning offers flexibility in adapting the pre-trained model to different tasks or domains by adjusting the training data and hyperparameters.\n",
    "\n",
    "3. **Regularization**: Fine-tuning provides a form of regularization that prevents overfitting by updating the model parameters while retaining the knowledge learned during pre-training.\n",
    "\n",
    "## Application to Project\n",
    "\n",
    "In our project, we can leverage transfer learning and fine-tuning to improve the overall effectiveness of sentiment analysis or text classification tasks. By initializing our models with pre-trained language models such as BERT or RoBERTa and fine-tuning them on our specific dataset containing tweets or text data, we can achieve better performance compared to training models from scratch.\n",
    "\n",
    "### Experiment Results\n",
    "\n",
    "In our limited experimentation, we observed the benefits of transfer learning and fine-tuning. By utilizing pre-trained BERT models and fine-tuning them on our dataset, we achieved improved accuracy and performance in classifying sentiments or predicting labels for tweets.\n",
    "\n",
    "Overall, transfer learning and fine-tuning offer a powerful approach to NLP tasks, allowing us to leverage the knowledge learned from large-scale pre-training and adapt it to our specific project requirements.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
